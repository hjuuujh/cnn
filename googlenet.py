# -*- coding: utf-8 -*-
"""GoogLeNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RLrvF5WnT3sWJ7tbfl_Tw0XU8Ua-bqGk
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as T
import torch.optim as optim
import torchvision.datasets as dset
from torch.utils.data import DataLoader
from google.colab import drive
drive.mount('/content/gdrive/')
from sklearn.metrics import confusion_matrix 
from sklearn.metrics import f1_score

# 1*1 5*5 -> 1*1 3*3 3*3 
# auxiliary classifier -> batch normalization
class Inception(nn.Module):
  
  def __init__(self,in_ch,out_ch11,mid_ch13,out_ch13,mid_ch133,out_ch133,out_pool):
      super(Inception,self).__init__()
        
      self.conv11 = nn.Sequential(
          nn.Conv2d(in_ch,out_ch11,1,1),
          nn.BatchNorm2d(out_ch11),
          nn.ReLU())
        
      self.conv13 = nn.Sequential(
          nn.Conv2d(in_ch,mid_ch13,kernel_size=1,stride=1),
          nn.BatchNorm2d(mid_ch13),
          nn.ReLU(),
          nn.Conv2d(mid_ch13,out_ch13,kernel_size=3,stride=1,padding=1),
          nn.BatchNorm2d(out_ch13),
          nn.ReLU())
    
      self.conv133 = nn.Sequential(
          nn.Conv2d(in_ch,mid_ch133,kernel_size=1,stride=1),
          nn.BatchNorm2d(mid_ch133),
          nn.ReLU(),
          nn.Conv2d(mid_ch133, out_ch133, kernel_size=3, padding=1),
          nn.BatchNorm2d(out_ch133),
          nn.ReLU(),
          nn.Conv2d(out_ch133,out_ch133, kernel_size=3, padding=1),
          nn.BatchNorm2d(out_ch133),
          nn.ReLU(),
        )
        
      self.pool_conv1 = nn.Sequential(
          nn.MaxPool2d(3,stride=1,padding=1),
          nn.Conv2d(in_ch,out_pool,kernel_size=1,stride=1),
          nn.BatchNorm2d(out_pool),
          nn.ReLU())
      
  def forward(self,x):
      conv11_out = self.conv11(x)
      conv13_out = self.conv13(x)
      conv133_out = self.conv133(x)
      pool_conv_out = self.pool_conv1(x)
      outputs = torch.cat([conv11_out,conv13_out,conv133_out,pool_conv_out],1) 
      return outputs

class GoogLeNet(nn.Module):  
  def __init__(self,num_classes):
    super(GoogLeNet,self).__init__()
    self.layer_1 = nn.Sequential(
        nn.Conv2d(1, 64, kernel_size=7,stride=2, padding=1),
        nn.MaxPool2d(3,stride=2,padding=1),
        nn.Conv2d(64, 192, kernel_size=3, stride=1,padding=1),
        nn.MaxPool2d(3,stride=2,padding=1)
    )

    self.layer_2 = nn.Sequential(
        Inception(192,64,96,128,16,32,32),
        Inception(256,128,128,192,32,96,64),
        nn.MaxPool2d(3,stride=2,padding=1)
    )

    self.layer_3 = nn.Sequential(
        Inception(480,192,96,208,16,48,64),
        Inception(512,160,112,224,24,64,64),
        Inception(512,128,128,256,24,64,64),
        Inception(512,112,144,288,32,64,64),
        Inception(528,256,160,320,32,128,128),
        nn.MaxPool2d(3,stride=2,padding=1)
    )

    self.layer_4 = nn.Sequential(
        Inception(832,256,160,320,32,128,128),
        Inception(832,384,192,384,48,128,128),
        nn.AvgPool2d(7, stride=1)
    )

    self.dropout = nn.Dropout2d(0.4)
    self.fc_layer = nn.Linear(1024,num_classes)

  def forward(self,x):
    out = self.layer_1(x)
    #print(out.shape)
    out = self.layer_2(out)
    #print(out.shape)
    out = self.layer_3(out)
    #print(out.shape)
    out = self.layer_4(out)
    #print(out.shape)
    out = self.dropout(out)
    #print(out.shape)
    out = out.view(out.size(0),-1)
    #print(out.shape)
    out = self.fc_layer(out)
    
    return out

if torch.cuda.is_available():
  device = torch.device('cuda:0')
else:
  print('x')
dtype = torch.float 
ltype = torch.long

###### Find LR that makes loss go down
path = '/content/gdrive/Shared drives/3조 통계적 기계 학습/data/'

trans = T.Compose([T.Resize([224 ,224]),
                   T.Grayscale(),
                   T.ToTensor(),
                   T.Normalize((0.5193,), (0.2560,))
            ])
trainset = dset.ImageFolder(path+'train/',transform=trans)
valset = dset.ImageFolder(path+'val/',transform=trans)
testset = dset.ImageFolder(path+'test/',transform=trans)

loader_train = DataLoader(trainset, shuffle=True, batch_size=64)
loader_val = DataLoader(valset, shuffle=True , batch_size=64)
loader_test = DataLoader(testset,shuffle=True, batch_size=64)

print(len(trainset))
print(len(valset))
print(len(testset))

####### confusiont matrix / f1 score

def check_accuracy(loader, model):
  if (loader==loader_val):
    print('Checking accuracy on val set') 
  num_correct = 0
  num_samples = 0
  model.eval() 
  true_list = []
  pred_list = []

  with torch.no_grad():
    for t,(x, y) in enumerate(loader):
      x = x.to(device=device, dtype=dtype)  
      y = y.to(device=device, dtype=ltype)
      scores = model(x)
      _, preds = scores.max(dim=1)
      num_correct += (preds == y).sum()
      num_samples += preds.size(0)
      true_list += y.to("cpu")
      pred_list += preds.to("cpu")
    
    acc = float(num_correct) / num_samples
    f1 = f1_score(true_list, pred_list, labels=[0,1,2,3], average=None)
    f1_covid19 = f1[1]
    cm = confusion_matrix(true_list, pred_list, labels=[0, 1, 2, 3])
    print(cm)
    
    print('Got [%d / %d] correct (%.2f) and COVID-19 F1-score = (%.2f)'  % (num_correct, num_samples, 100 * acc, f1_covid19))
  return acc, preds, f1_covid19

def adjust_learning_rate(optimizer, lrd, epoch, schedule):
  if epoch in schedule:
    for param_group in optimizer.param_groups:
      print('lr decay from {} to {}'.format(param_group['lr'], param_group['lr'] * lrd))
      param_group['lr'] *= lrd

def train_part(model, optimizer, epochs=1, learning_rate_decay=.1, schedule=[], verbose=True):

  model = model.to(device=device)
  num_iters = epochs * len(loader_train)
  if verbose:
    num_prints = num_iters // print_every + 1
  else:
    num_prints = epochs
  
  acc_history = torch.zeros(num_prints, dtype=torch.float)
  iter_history = torch.zeros(num_prints, dtype=torch.long)
  f1_history = torch.zeros(num_prints, dtype=torch.float) 
  train_loss = []
  
  for e in range(epochs):
    adjust_learning_rate(optimizer, learning_rate_decay, e, schedule)
    for t, (x, y) in enumerate(loader_train):
      model.train() 
      
      x = x.to(device=device, dtype=dtype) 
      y = y.to(device=device, dtype=ltype)
      
      scores = model(x)

      loss = F.cross_entropy(scores, y, weight=weight_tensor)
      print('Iteration %d, loss = %.4f' % (t, loss.item()))
      train_loss.append(loss)
      loss.backward()
      optimizer.step()
      optimizer.zero_grad()
      tt = t + e * len(loader_train)

      if verbose and (tt % print_every == 0 or (e == epochs-1 and t == len(loader_train)-1)):
        print('Epoch %d, Iteration %d, loss = %.4f' % (e, tt, loss.item()))
        acc, train_preds, f1_covid19 = check_accuracy(loader_val, model)
        acc_history[tt // print_every] = acc
        iter_history[tt // print_every] = tt
        f1_history[tt // print_every] = f1_covid19
        print()
      elif not verbose and (t == len(loader_train)-1):
        print('Epoch %d, Iteration %d, loss = %.4f' % (e, tt, loss.item()))
        acc, train_preds, f1_covid19 = check_accuracy(loader_val, model)
        acc_history[e] = acc
        iter_history[e] = tt
        f1_history[e] = f1_covid19
        print()
      
          
  return acc_history,iter_history, f1_history, train_loss, model

print_every=50
learning_rate = 1e-4
weight_decay = 1e-4
s = [10,15,20,25]
 
# 1/2035, 1/149, 1/1042, 1/1049
weight_tensor = torch.tensor([0.0005, 0.0067, 0.0001, 0.0001], device = device, dtype=dtype)
 
model = GoogLeNet(4)
optimizer = optim.Adam(model.parameters(), lr = learning_rate, weight_decay = weight_decay)
acc_history, iter_history, f1_history, train_loss, model = train_part(model, optimizer, schedule=s, epochs=30)

import matplotlib.pyplot as plt
import numpy as np

acc_history_dict = {}
iter_history_dict = {}
f1_history_dict = {}

acc_history_dict['googlenet'] = acc_history
f1_history_dict['googlenet'] = f1_history
iter_history_dict['googlenet'] = iter_history

plt.title('Train loss')
plt.plot(np.array(train_loss), 'o')
plt.legend('googlenet', loc='upper left')
plt.xlabel('iterations')
plt.ylabel('loss')
plt.gcf().set_size_inches(9, 4)
plt.show()

plt.title('Val accuracies')
plt.plot(iter_history_dict['googlenet'], acc_history_dict['googlenet'], '-o')
plt.legend('googlenet', loc='upper left')
plt.xlabel('iterations')
plt.ylabel('accuracy')
plt.gcf().set_size_inches(9, 4)
plt.show()

plt.title('[COVID19] Val f1_score ')
plt.plot(iter_history_dict['googlenet'], f1_history_dict['googlenet'], '-o')
plt.legend('googlenet', loc='upper left')
plt.xlabel('iterations')
plt.ylabel('covid19 f1_score')
plt.gcf().set_size_inches(9, 4)
plt.show()

#def check_accuracy(loader, model):
print('Checking accuracy on test set') 
num_correct = 0
num_samples = 0
model.eval() 
true_list = []
pred_list = []

with torch.no_grad():
  for t,(x, y) in enumerate(loader_test):
    x = x.to(device=device, dtype=dtype)  
    y = y.to(device=device, dtype=ltype)
    
    scores = model(x)
    _, preds = scores.max(dim=1)
    num_correct += (preds == y).sum()
    num_samples += preds.size(0)
    true_list += y.to("cpu")
    pred_list += preds.to("cpu")
  
  acc = float(num_correct) / num_samples
  f1 = f1_score(true_list, pred_list, labels=[0,1,2,3], average=None)
  f1_covid19 = f1[1]
  cm = confusion_matrix(true_list, pred_list, labels=[0, 1, 2, 3])
  print(cm) 
  print('Got [%d / %d] correct (%.2f) and COVID-19 F1-score = (%.2f)'  % (num_correct, num_samples, 100 * acc, f1_covid19))

import matplotlib.pylab as plt
import numpy as np
import itertools
from sklearn.metrics import recall_score

cmat = np.round_(cm/cm.sum(axis=1)[:,None],3)
label = ['Bacteria', 'COVID-19', 'Normal', 'Virus' ]

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    thresh = (cm.max() / 2.)
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

plt.figure()
plot_confusion_matrix(cmat, classes=label,
                      title='Confusion matrix')

plt.show()